{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æœºå™¨å­¦ä¹ ä¸­çš„çº¿æ€§å›å½’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å•ä¸€ç‰¹å¾çš„çº¿æ€§å›å½’\n",
    "\n",
    "ç›®çš„ï¼šæ ¹æ®æä¾›çš„çœŸå®æ•°æ®ï¼ˆæˆ¿å±‹é¢ç§¯ä¸æˆ¿å±‹ä»·æ ¼ï¼‰ï¼Œå®ç°ä»·æ ¼é¢„æµ‹æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®æˆ¿å±‹é¢ç§¯é¢„æµ‹å¯èƒ½çš„æˆ¿å±‹ä»·æ ¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¼å…¥å·¥å…·åº“\n",
    "\n",
    "NumPyï¼Œä¸€ä¸ªæµè¡Œçš„ç§‘å­¦è®¡ç®—åº“\n",
    "Matplotlibï¼Œä¸€ä¸ªæµè¡Œçš„æ•°æ®ç»˜å›¾åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®­ç»ƒæ•°æ®é›†\n",
    "æˆ¿å±‹é¢ç§¯ä¸æˆ¿å±‹ä»·æ ¼çš„ç®€å•æ•°æ®é›†\n",
    "\n",
    "| é¢ç§¯ (100 å¹³æ–¹ç±³)     | ä»·æ ¼ (ä¸‡) |\n",
    "| -------------------| ------------------------ |\n",
    "| 1.0               | 250                      |\n",
    "| 1.7               | 300                      |\n",
    "| 2.0               | 480                      |\n",
    "| 2.5               | 430                      |\n",
    "| 3.0               | 630                      |\n",
    "| 3.2               | 730                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_train = np.array([250, 300, 480, 430, 630, 730])\n",
    "print(f\"x_train = {x_train}\")\n",
    "print(f\"y_train = {y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å•ä¸€ç‰¹å¾çš„æ¨¡å‹è¡¨ç¤ºæ³• \n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b $$\n",
    "\n",
    "æ¨¡å‹å®ç°å¦‚ä¸‹ï¼šcompute_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_output(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction of a linear model\n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      f_wb (ndarray (m,)): model prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    f_wb = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f_wb[i] = w * x[i] + b\n",
    "        \n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç»˜åˆ¶æ¨¡å‹å‡½æ•°\n",
    "å‡è®¾æˆ‘ä»¬æ¨¡å‹çš„å‚æ•°wä¸º100ï¼Œbä¸º200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 100\n",
    "b = 200\n",
    "print(f\"w: {w}\")\n",
    "print(f\"b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»˜åˆ¶æ¨¡å‹çš„é¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_f_wb = compute_model_output(x_train, w, b,)\n",
    "\n",
    "# Plot our model prediction\n",
    "plt.plot(x_train, tmp_f_wb, c='b',label='model prediction')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(x_train, y_train, marker='x', c='r',label='Real values')\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"housing price\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Price (10000)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Area (100 square meters)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¨¡å‹é¢„æµ‹æˆ¿å±‹ä»·æ ¼\n",
    "120å¹³æ–¹ç±³çš„æˆ¿å±‹çš„é¢„æµ‹ä»·æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 100                         \n",
    "b = 200    \n",
    "x_i = 1.2\n",
    "cost_120sqft = w * x_i + b    \n",
    "\n",
    "print(f\"ï¿¥{cost_120sqft:.0f} ä¸‡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯»æ‰¾æœ€ä½³wå’Œb\n",
    "æ‰¾åˆ°æœ€åˆé€‚çš„wä¸bï¼Œèƒ½å¤Ÿæœ€å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”èƒ½ç›¸å¯¹å‡†ç¡®åœ°é¢„æµ‹æˆ¿å±‹ä»·æ ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æˆæœ¬å‡½æ•°\n",
    "é€šè¿‡æˆæœ¬å‡½æ•°è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®è·\n",
    "\n",
    "æˆæœ¬å‡½æ•°è¡¨ç¤ºæ³•\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    cost_sum = 0 \n",
    "    for i in range(m): \n",
    "        f_wb = w * x[i] + b   \n",
    "        cost = (f_wb - y[i]) ** 2  \n",
    "        cost_sum = cost_sum + cost  \n",
    "    total_cost = (1 / (2 * m)) * cost_sum  \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä½¿ç”¨å·²ç»é¢„å…ˆè°ƒè¯•å¥½çš„æ–¹æ³•æ¥å±•ç¤ºä¸åŒw,bçš„æˆæœ¬å€¼ï¼Œå…¶ä¸­plt_stationaryæ–¹æ³•éœ€è¦ä½¿ç”¨æˆæœ¬å‡½æ•°æ–¹æ³•compute_costï¼Œå…·ä½“çš„ä»£ç åç»­å±•å¼€è¯´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_stationary\n",
    "\n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¤§çº¦ğ‘¤=209çš„å€¼ï¼Œğ‘=2.4 æä¾›ä½æˆæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¢¯åº¦ä¸‹é™å¯»æ‰¾æœ€ä½³w,b\n",
    "ç›®æµ‹å€¼æ€»å½’æ˜¯ä¸å¤ªå‡†ç¡®çš„ï¼Œä¸‹é¢å°†é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è®¡ç®—æœ€ä½³w,bã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡è®¡ç®—ä¸Šé¢æ›²é¢å›¾åœ¨æŸä¸€ä¸ªç‚¹çš„åå¯¼æ•°ï¼Œè®¾ç½®åˆé€‚çš„å­¦ä¹ ç‡ï¼Œé‡å¤è®¡ç®—ï¼Œç›´åˆ°æ”¶æ•›ã€‚\n",
    "\n",
    "æ¢¯åº¦ä¸‹é™çš„æè¿°ï¼š\n",
    "$$\\begin{align*} \\text{é‡å¤}&\\text{ ç›´åˆ°æ”¶æ•›:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "åå¯¼æ•°è®¡ç®—ï¼š\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \n",
    "\\; \\newline \n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\; \\newline \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ä»£ç å®ç°åå¯¼æ•°çš„è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»˜å›¾å±•ç¤ºä¸‹åå¯¼æ•°çš„è®¡ç®—è¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_gradients\n",
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢æ˜¯ä¸€ä¸ªæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢ä½¿ç”¨gradient_descentå‡½æ•°åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‰¾åˆ°ğ‘¤çš„æœ€ä½³å€¼ä»¥åŠğ‘çš„æœ€ä½³å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ä¸ªå€¼ï¼Œwä¸º209.3590ï¼Œbä¸º2.4325ï¼›å…¶å®è¿™ä¸ªå¹¶ä¸æ˜¯ç»å¯¹çš„æœ€ä½³å€¼ï¼Œè¿˜å–å†³äºå­¦ä¹ ç‡çš„å¤§å°ï¼Œä¸åŒçš„å­¦ä¹ ç‡æœ€ç»ˆè®¡ç®—å‡ºæ¥çš„wï¼Œbçš„å€¼ä¹Ÿä¼šæœ‰å·®å¼‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ€»ç»“\n",
    "å•ä¸€ç‰¹å¾çš„çº¿æ€§å›å½’é€šè¿‡æˆæœ¬å‡½æ•°ç»“åˆæ¢¯åº¦ä¸‹é™æ³•ï¼Œè®¡ç®—æœ€ä¼˜wï¼Œbçš„å€¼ã€‚ä»è€Œè®¡ç®—å‡ºèƒ½å¾ˆå¥½åœ°é¢„æµ‹ç»“æœçš„æ¨¡å‹å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤šå…ƒç‰¹å¾çš„çº¿æ€§å›å½’\n",
    "åœ¨å®é™…åœºæ™¯ä¸­å¾ˆå°‘æ ¹æ®å•ä¸€çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹ï¼Œä¸‹é¢ä»‹ç»å¤šå…ƒç‰¹å¾çš„çº¿æ€§å›å½’é¢„æµ‹ï¼Œæ ¹æ®æä¾›çš„çœŸå®æ•°æ®ï¼ˆæˆ¿å±‹é¢ç§¯ã€å§å®¤æ•°é‡ã€æ¥¼æˆ¿å±‚æ•°ã€æˆ¿å±‹å¹´é¾„ä¸æˆ¿å±‹ä»·æ ¼ï¼‰ï¼Œå®ç°ä»·æ ¼é¢„æµ‹æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®å¤šä¸ªç‰¹å¾ï¼ˆé¢ç§¯ã€å§å®¤æ•°é‡ã€å±‚æ•°ã€æˆ¿å±‹å¹´é¾„ï¼‰é¢„æµ‹å¯èƒ½çš„æˆ¿å±‹ä»·æ ¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤šå…ƒç‰¹å¾çš„è®­ç»ƒæ•°æ®é›†\n",
    "æˆ¿å±‹é¢ç§¯ï¼Œå§å®¤æ•°é‡ï¼Œæ¥¼æˆ¿å±‚æ•°ï¼Œæˆ¿å±‹å¹´é¾„ä¸æˆ¿å±‹ä»·æ ¼çš„æ•°æ®é›†\n",
    "\n",
    "| é¢ç§¯ | å§å®¤æ•°é‡  | æ¥¼æˆ¿å±‚æ•° | æˆ¿å±‹å¹´é¾„ | ä»·æ ¼ï¼ˆä¸‡ï¼‰  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤šå…ƒæ¨¡å‹è¡¨ç¤ºæ³•\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b $$\n",
    "\n",
    "æ¨¡å‹ä»£ç çš„ç®€å•å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆå§‹åŒ–wï¼Œbï¼Œè¿›è¡Œç²—ç•¥é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083 # éšæœºé€‰æ‹©çš„å‚æ•°\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618]) # éšæœºé€‰æ‹©çš„å‚æ•°\n",
    "x_vec = X_train[0,:]\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"é¢„æµ‹ç»“æœ: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨NumPyçš„å‘é‡è¿ç®—æ–¹æ³•åŠ é€Ÿè®¡ç®—\n",
    "å‘é‡è¿ç®—å¯ä»¥é€šè¿‡å¹¶è¡Œè®¡ç®—ï¼Œæé«˜è®¡ç®—é€Ÿåº¦ï¼›\n",
    "ç”¨å‘é‡è¿ç®—é‡å†™predict_single_loopæ–¹æ³•ï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆå§‹åŒ–wï¼Œbï¼Œé€šè¿‡å‘é‡è¿ç®—è¿›è¡Œé¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f\"é¢„æµ‹ç»“æœ: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤šå˜é‡çš„æˆæœ¬å‡½æ•°è¡¨ç¤ºæ³•\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 $$ \n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b   $$ \n",
    "\n",
    "ä»£ç å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost\n",
    "\n",
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤šå˜é‡æ¢¯åº¦ä¸‹é™\n",
    "$$\\begin{align*} \\text{é‡å¤}&\\text{ ç›´åˆ°æ”¶æ•›:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "åå¯¼æ•°å…¬å¼æ˜¯é€šè¿‡å¾®ç§¯åˆ†è®¡ç®—å‡ºæ¥çš„ã€‚\n",
    "\n",
    "$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ ä¸$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw\n",
    "\n",
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤šå˜é‡æ¢¯åº¦ä¸‹é™ä»£ç å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¢¯åº¦ä¸‹é™è®¡ç®—æœ€ä½³wï¼Œb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç‰¹å¾ç¼©æ”¾\n",
    "åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæœ€ç»ˆè®¡ç®—å‡ºæ¥çš„æœ€ä½³wï¼Œbä»¥åŠè®¡ç®—çš„æ¬¡æ•°ï¼Œä¸å­¦ä¹ ç‡ä»¥åŠæˆæœ¬å‡½æ•°è§†å›¾çš„â€œå¹³æ»‘â€ç¨‹åº¦æœ‰å¾ˆå¤§å…³ç³»ã€‚\n",
    "å­¦ä¹ ç‡è¿‡å¤§ï¼Œæˆ–è€…ç‰¹å¾æ•°å€¼å·®å¼‚å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ— æ³•å¿«é€Ÿæ‰¾åˆ°æœ€ä½³wï¼Œbå€¼ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šä¸‹é¢å°†æä¾›ä¸€äº›æˆ¿å±‹ç‰¹å¾ä¸ä»·æ ¼çš„çœŸå®æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_multi import  load_house_data, run_gradient_descent \n",
    "from lab_utils_multi import  norm_plot, plt_equal_scale, plot_cost_i_w\n",
    "from lab_utils_common import dlc\n",
    "\n",
    "X_train, y_train = load_house_data()\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ç»˜åˆ¶æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡ä»·æ ¼çš„å…³ç³»å›¾ï¼Œå¯ä»¥è¡¨æ˜å“ªäº›ç‰¹å¾å¯¹ä»·æ ¼çš„å½±å“æœ€å¤§ã€‚å¦‚ä¸Šæ‰€è¿°ï¼Œå°ºå¯¸çš„å¢åŠ ä¹Ÿä¼šå¢åŠ ä»·æ ¼ã€‚å§å®¤å’Œåœ°æ¿ä¼¼ä¹å¯¹ä»·æ ¼æ²¡æœ‰å¤ªå¤§å½±å“ã€‚æ–°æˆ¿å­çš„ä»·æ ¼æ¯”æ—§æˆ¿å­é«˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è§£å†³æŸäº›ç‰¹å¾å¯¹æ¨¡å‹çš„å½±å“è¿œè¿œå¤§äºå…¶å®ƒç‰¹å¾çš„æƒ…å†µã€‚\n",
    "ä¸‹é¢ä»‹ç»ä¸¤ç§ç‰¹å¾ç¼©æ”¾çš„æ–¹æ³•ï¼šä¸€ã€å‡å€¼å½’ä¸€åŒ–ï¼ŒäºŒã€Z-scoreæ ‡å‡†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å‡å€¼å½’ä¸€åŒ–:\n",
    "\n",
    "$$x_i := \\dfrac{x_i - \\mu_i}{max - min} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-scoreæ ‡å‡†åŒ–:\n",
    "\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} $$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Z-scoreæ ‡å‡†åŒ–ä»£ç å®ç°å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    " \n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "\n",
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å°†è®­ç»ƒæ•°æ®å½’ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨å½’ä¸€åŒ–æ•°æ®é‡æ–°è¿è¡Œæ¢¯åº¦ä¸‹é™ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¼©æ”¾çš„ç‰¹å¾å¯ä»¥æ›´å¿«åœ°è·å¾—éå¸¸å‡†ç¡®çš„ç»“æœï¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç‰¹å¾å·¥ç¨‹å’Œå¤šé¡¹å¼å›å½’\n",
    "\n",
    "æˆ¿ä»·å¾€å¾€ä¸å±…ä½é¢ç§¯ä¸æˆçº¿æ€§å…³ç³»ï¼Œè¿™æ ·ä¸èƒ½å¾ˆå¥½çš„é¢„æµ‹éå¸¸å°æˆ–éå¸¸å¤§çš„æˆ¿å­çš„ä»·æ ¼ï¼Œæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨çº¿æ€§å›å½’æœºåˆ¶æ¥æ‹Ÿåˆè¿™æ¡æ›²çº¿ï¼Ÿ\n",
    "\n",
    "ä¸‹é¢ç”¨ç»˜åˆ¶æ•°æ®è§†å›¾å±•ç¤º\n",
    "ä½¿ç”¨ç‰¹å¾$x$ä»¥åŠç»“æœä¸º$y = 1+x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_multi import run_gradient_descent_feng\n",
    "# create target data\n",
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "model_w,model_b = run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-2)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"no feature engineering\")\n",
    "plt.plot(x,X@model_w + model_b, label=\"Predicted Value\");  plt.xlabel(\"X\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ˜¾è€Œæ˜“è§ï¼Œæ¨¡å‹å¹¶ä¸èƒ½å¾ˆå¥½çš„æ‹Ÿåˆæ›²çº¿ã€‚æˆ‘ä»¬éœ€è¦ç±»ä¼¼$y= w_0x_0^2 + b$æˆ–è€…å¤šé¡¹å¼ç‰¹å¾ï¼Œæˆ‘ä»¬ä½¿ç”¨$x^2$ä½œä¸ºç‰¹å¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 20, 1)\n",
    "y = 1 + x**2\n",
    "\n",
    "# Engineer features \n",
    "X = x**2      #<-- added engineered feature\n",
    "\n",
    "X = X.reshape(-1, 1)  #X should be a 2-D Matrix\n",
    "model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)\n",
    "\n",
    "plt.scatter(x, y, marker='x', c='r', label=\"Actual Value\"); plt.title(\"Added x**2 feature\")\n",
    "plt.plot(x, np.dot(X,model_w) + model_b, label=\"Predicted Value\"); plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡ ä¹å®Œç¾å¥‘åˆã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
